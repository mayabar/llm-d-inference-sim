/*
Copyright 2025 The llm-d-inference-sim Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

// Package vllmsim implements the vLLM simulator.
package llmdinferencesim

import (
	"bufio"
	"context"
	"encoding/json"
	"fmt"
	"net"
	"strconv"
	"strings"
	"time"

	"github.com/buaazp/fasthttprouter"
	"github.com/prometheus/client_golang/prometheus/promhttp"
	"github.com/valyala/fasthttp"
	"github.com/valyala/fasthttp/fasthttpadaptor"

	"github.com/llm-d/llm-d-inference-sim/pkg/common"
	"github.com/llm-d/llm-d-inference-sim/pkg/common/logging"
	openaiserverapi "github.com/llm-d/llm-d-inference-sim/pkg/openai-server-api"
	vllmapi "github.com/llm-d/llm-d-inference-sim/pkg/vllm-api"
)

func (s *VllmSimulator) newListener() (net.Listener, error) {
	listener, err := net.Listen("tcp4", fmt.Sprintf(":%d", s.context.config.Port))
	if err != nil {
		return nil, err
	}
	return listener, nil
}

// startServer starts http/https server on port defined in command line
func (s *VllmSimulator) startServer(ctx context.Context, listener net.Listener) error {
	r := fasthttprouter.New()

	// support completion APIs
	r.POST("/v1/chat/completions", s.HandleChatCompletions)
	r.POST("/v1/completions", s.HandleTextCompletions)
	// supports /models API
	r.GET("/v1/models", s.HandleModels)
	// support load/unload of lora adapter
	r.POST("/v1/load_lora_adapter", s.HandleLoadLora)
	r.POST("/v1/unload_lora_adapter", s.HandleUnloadLora)
	// supports /metrics prometheus API
	r.GET("/metrics", fasthttpadaptor.NewFastHTTPHandler(promhttp.HandlerFor(s.context.metrics.registry, promhttp.HandlerOpts{})))
	// supports standard Kubernetes health and readiness checks
	r.GET("/health", s.HandleHealth)
	r.GET("/ready", s.HandleReady)
	r.POST("/tokenize", s.HandleTokenize)
	r.POST("/sleep", s.HandleSleep)
	r.POST("/wake_up", s.HandleWakeUp)
	r.GET("/is_sleeping", s.HandleIsSleeping)

	server := &fasthttp.Server{
		ErrorHandler: s.HandleError,
		Handler:      r.Handler,
		Logger:       s,
	}

	if err := s.configureSSL(server); err != nil {
		return err
	}

	// Start server in a goroutine
	serverErr := make(chan error, 1)
	go func() {
		if s.context.config.SSLEnabled() {
			s.context.logger.V(logging.INFO).Info("Server starting", "protocol", "HTTPS", "port", s.context.config.Port)
			serverErr <- server.ServeTLS(listener, "", "")
		} else {
			s.context.logger.V(logging.INFO).Info("Server starting", "protocol", "HTTP", "port", s.context.config.Port)
			serverErr <- server.Serve(listener)
		}
	}()

	// Wait for either context cancellation or server error
	select {
	case <-ctx.Done():
		s.context.logger.V(logging.INFO).Info("Shutdown signal received, shutting down server gracefully")

		// Gracefully shutdown the server
		if err := server.Shutdown(); err != nil {
			s.context.logger.Error(err, "error during server shutdown")
			return err
		}

		s.context.logger.V(logging.INFO).Info("Server stopped")
		return nil

	case err := <-serverErr:
		if err != nil {
			s.context.logger.Error(err, "server failed")
		}
		return err
	}
}

// getRequestID retrieves the request ID from the X-Request-Id header or generates a new one if not present
func (s *VllmSimulator) getRequestID(ctx *fasthttp.RequestCtx) string {
	if s.context.config.EnableRequestIDHeaders {
		requestID := string(ctx.Request.Header.Peek(requestIDHeader))
		if requestID != "" {
			return requestID
		}
	}
	return s.context.random.GenerateUUIDString()
}

// HandleChatCompletions http handler for /v1/chat/completions
func (s *VllmSimulator) HandleChatCompletions(ctx *fasthttp.RequestCtx) {
	s.handleHTTP(&chatCompletionRequest{}, ctx)
}

// HandleTextCompletions http handler for /v1/completions
func (s *VllmSimulator) HandleTextCompletions(ctx *fasthttp.RequestCtx) {
	s.handleHTTP(&textCompletionRequest{}, ctx)
}

func (s *VllmSimulator) handleHTTP(req request, ctx *fasthttp.RequestCtx) {
	isStream, reqCtx, channel, err, errInjected := s.handleHTTPRequest(req, ctx)
	if err != nil {
		s.sendError(ctx, err, errInjected)
		return
	}

	s.context.logger.V(logging.DEBUG).Info("Received", "new HTTP", req.asString())

	ctx.SetStatusCode(fasthttp.StatusOK)

	// Add pod and namespace information to response headers for testing/debugging
	if s.context.pod != "" {
		ctx.Response.Header.Add(podHeader, s.context.pod)
		ctx.Response.Header.Add(portHeader, strconv.Itoa(s.context.config.Port))
	}
	if s.context.namespace != "" {
		ctx.Response.Header.Add(namespaceHeader, s.context.namespace)
	}
	if s.context.config.EnableRequestIDHeaders {
		ctx.Response.Header.Add(requestIDHeader, reqCtx.request().GetRequestID())
	}

	if isStream {
		ctx.SetContentType("text/event-stream")
		s.sendStream(ctx, channel)
	} else {
		ctx.SetContentType("application/json")
		s.sendNonStream(ctx, channel)
	}
}

func (s *VllmSimulator) sendNonStream(ctx *fasthttp.RequestCtx, channel chan *responseInfo) {
	tokens := openaiserverapi.Tokenized{
		Tokens:  make([]uint32, 0),
		Strings: make([]string, 0),
	}

	var respCtx responseContext
	for response := range channel {
		if response.err != nil {
			s.sendError(ctx, response.err, false)
			return
		}

		if response.tokens != nil {
			tokens.Append(*response.tokens)
		}
		respCtx = response.respCtx
	}

	defer respCtx.done()
	resp := respCtx.createResponse(&tokens)
	data, err := json.Marshal(resp)
	if err != nil {
		err := openaiserverapi.NewError("Response body creation failed, "+err.Error(), fasthttp.StatusInternalServerError, nil)
		s.sendError(ctx, &err, false)
		return
	}
	ctx.Response.SetBody(data)
	s.responseSentCallback(respCtx.requestContext(), respCtx.displayModel())
}

func (s *VllmSimulator) sendStream(ctx *fasthttp.RequestCtx, channel chan *responseInfo) {
	ctx.SetBodyStreamWriter(func(w *bufio.Writer) {
		first := true
		var respCtx responseContext
		var lastToolCall *openaiserverapi.ToolCall
		var toolCallIndex int
		for response := range channel {
			if response.err != nil {
				ctx.Error(response.err.Message, response.err.Code)
				return
			}
			if first {
				respCtx = response.respCtx
				respCtx.setCreationTime(time.Now().Unix())
			}

			// nolint
			if response.tokens != nil {
				// in chat completion first chunk contains the role
				if first {
					chunk := respCtx.createFirstCompletionChunk()
					if chunk != nil {
						if err := s.sendChunk(w, chunk, ""); err != nil {
							s.chunkSendFailed(ctx, respCtx, "Sending first stream chunk failed, ", err)
							return
						}
					}
					first = false
				}
				if response.toolCall != nil {
					if lastToolCall != response.toolCall {
						toolCallIndex = 0
					} else {
						toolCallIndex++
					}
					if ok := s.sendStreamedTools(respCtx, ctx, w, response.tokens.Strings, response.toolCall, toolCallIndex); !ok {
						return
					}
					lastToolCall = response.toolCall
				} else {
					chunk := respCtx.createCompletionChunk(response.tokens.Strings, nil, "", nil)
					if err := s.sendChunk(w, chunk, ""); err != nil {
						s.chunkSendFailed(ctx, respCtx, "Sending stream chunk failed, ", err)
						return
					}
				}
			} else if respCtx.finishReason() != nil && *respCtx.finishReason() == common.CacheThresholdFinishReason {
				// No tokens to stream but we still need to emit a finish chunk for cache_threshold
				chunk := respCtx.createCompletionChunk(nil, nil, "", respCtx.finishReason())
				if err := s.sendChunk(w, chunk, ""); err != nil {
					s.chunkSendFailed(ctx, respCtx, "Sending finish chunk failed, ", err)
					return
				}
			} else {
				ctx.Error("unexpected response part in streaming", fasthttp.StatusInternalServerError)
				respCtx.done()
				return
			}
		}

		// send the last chunk if finish reason is stop
		if *respCtx.finishReason() == common.StopFinishReason {
			chunk := respCtx.createCompletionChunk(nil, nil, "", respCtx.finishReason())
			if err := s.sendChunk(w, chunk, ""); err != nil {
				s.chunkSendFailed(ctx, respCtx, "Sending last stream chunk failed, ", err)
				return
			}
		}

		// send usage
		if respCtx.sendUsageData() {
			chunk := respCtx.createUsageChunk()
			if err := s.sendChunk(w, chunk, ""); err != nil {
				s.chunkSendFailed(ctx, respCtx, "Sending usage chunk failed, ", err)
				return
			}
		}

		// finish sse events stream
		if err := s.sendChunk(w, nil, "[DONE]"); err != nil {
			s.chunkSendFailed(ctx, respCtx, "Sending last stream chunk failed, ", err)
			return
		}
		s.responseSentCallback(respCtx.requestContext(), respCtx.displayModel())
		respCtx.done()
	})
}

func (s *VllmSimulator) chunkSendFailed(ctx *fasthttp.RequestCtx, respCtx responseContext, msg string, err error) {
	ctx.Error(msg+err.Error(), fasthttp.StatusInternalServerError)
	respCtx.done()
}

func (s *VllmSimulator) sendStreamedTools(respCtx responseContext, ctx *fasthttp.RequestCtx, w *bufio.Writer, tokens []string,
	tc *openaiserverapi.ToolCall, index int) bool {
	tokensStr := strings.Join(tokens, "")

	toolChunkInsert := &openaiserverapi.ToolCall{
		ID:    tc.ID,
		Type:  tc.Type,
		Index: tc.Index,
		Function: openaiserverapi.FunctionCall{
			Arguments: tokensStr,
		},
	}
	if index == 0 {
		toolChunkInsert.Function.Name = tc.Function.Name
	}

	var chunk openaiserverapi.CompletionRespChunk
	var finishReasonToSend *string
	if index == tc.Function.TokenizedArguments().Length()-1 && (*respCtx.finishReason() == common.LengthFinishReason ||
		*respCtx.finishReason() == common.ToolsFinishReason ||
		*respCtx.finishReason() == common.CacheThresholdFinishReason) {
		finishReasonToSend = respCtx.finishReason()
	}
	chunk = respCtx.createCompletionChunk(nil, toolChunkInsert, "", finishReasonToSend)
	if err := s.sendChunk(w, chunk, ""); err != nil {
		ctx.Error("Sending stream chunk failed, "+err.Error(), fasthttp.StatusInternalServerError)
		return false
	}
	return true
}

// sendChunk send a single token chunk in a streamed completion API response,
// receives either a completionRespChunk or a string with the data to send.
func (s *VllmSimulator) sendChunk(w *bufio.Writer, chunk openaiserverapi.CompletionRespChunk, dataString string) error {
	if dataString == "" {
		data, err := json.Marshal(chunk)
		if err != nil {
			return err
		}
		dataString = string(data)
	}

	_, err := fmt.Fprintf(w, "data: %s\n\n", dataString)
	if err != nil {
		return err
	}

	err = w.Flush()
	if err != nil {
		return err
	}

	return nil
}

func (s *VllmSimulator) sendError(ctx *fasthttp.RequestCtx, err *openaiserverapi.Error, isInjected bool) {
	if isInjected {
		s.context.logger.V(logging.TRACE).Info("Injecting failure", "type", err.Type, "message", err.Message)
	} else {
		s.context.logger.Error(nil, err.Message)
	}

	errorResp := openaiserverapi.ErrorResponse{
		Error: *err,
	}

	data, jsonErr := json.Marshal(errorResp)
	if jsonErr != nil {
		ctx.Error(jsonErr.Error(), fasthttp.StatusInternalServerError)
	} else {
		ctx.SetContentType("application/json")
		ctx.SetStatusCode(err.Code)
		ctx.SetBody(data)
	}
}

// readTokenizeRequest reads and parses data from the body of the given request
func (s *VllmSimulator) readTokenizeRequest(ctx *fasthttp.RequestCtx) (*vllmapi.TokenizeRequest, error) {
	var tokenizeReq vllmapi.TokenizeRequest
	if err := json.Unmarshal(ctx.Request.Body(), &tokenizeReq); err != nil {
		s.context.logger.Error(err, "failed to unmarshal tokenize request body")
		return nil, err
	}
	return &tokenizeReq, nil
}

// HandleTokenize http handler for /tokenize
func (s *VllmSimulator) HandleTokenize(ctx *fasthttp.RequestCtx) {
	s.context.logger.V(logging.TRACE).Info("Tokenize request received")
	req, err := s.readTokenizeRequest(ctx)
	if err != nil {
		s.context.logger.Error(err, "failed to read and parse tokenize request body")
		ctx.Error("Failed to read and parse tokenize request body, "+err.Error(), fasthttp.StatusBadRequest)
		return
	}

	// Check that the request has only one input to tokenize
	if req.Prompt != "" && req.Messages != nil {
		err := openaiserverapi.NewError("both prompt and messages fields in tokenize request",
			fasthttp.StatusBadRequest, nil)
		s.sendError(ctx, &err, false)
		return
	}
	// Model is optional, if not set, the model from the configuration will be used
	tokens, _, err := s.context.tokenizer.Encode(req.GetPrompt(), req.Model)
	if err != nil {
		s.context.logger.Error(err, "failed to tokenize")
		ctx.Error("Failed to tokenize, "+err.Error(), fasthttp.StatusInternalServerError)
		return
	}
	resp := vllmapi.TokenizeResponse{
		Count:       len(tokens),
		Tokens:      tokens,
		MaxModelLen: s.context.config.MaxModelLen,
	}
	data, err := json.Marshal(resp)
	if err != nil {
		ctx.Error("Response body creation failed, "+err.Error(), fasthttp.StatusInternalServerError)
		return
	}
	ctx.Response.Header.SetContentType("application/json")
	ctx.Response.Header.SetStatusCode(fasthttp.StatusOK)
	ctx.Response.SetBody(data)
}

func (s *VllmSimulator) HandleLoadLora(ctx *fasthttp.RequestCtx) {
	s.context.logger.V(logging.DEBUG).Info("Load lora request received")
	s.context.loadLoraAdaptor(ctx)
}

func (s *VllmSimulator) HandleUnloadLora(ctx *fasthttp.RequestCtx) {
	s.context.logger.V(logging.DEBUG).Info("Unload lora request received")
	s.context.unloadLoraAdaptor(ctx)
}

// HandleModels handles /v1/models request according the data stored in the simulator
func (s *VllmSimulator) HandleModels(ctx *fasthttp.RequestCtx) {
	s.context.logger.V(logging.TRACE).Info("/models request received")
	modelsResp := s.createModelsResponse()

	data, err := json.Marshal(modelsResp)
	if err != nil {
		s.context.logger.Error(err, "failed to marshal models response")
		ctx.Error("Failed to marshal models response, "+err.Error(), fasthttp.StatusInternalServerError)
		return
	}

	ctx.Response.Header.SetContentType("application/json")
	ctx.Response.Header.SetStatusCode(fasthttp.StatusOK)
	ctx.Response.SetBody(data)
}

func (s *VllmSimulator) HandleError(_ *fasthttp.RequestCtx, err error) {
	s.context.logger.Error(err, "vLLM server error")
}

// HandleHealth http handler for /health
func (s *VllmSimulator) HandleHealth(ctx *fasthttp.RequestCtx) {
	s.context.logger.V(logging.TRACE).Info("Health request received")
	ctx.Response.Header.SetContentType("application/json")
	ctx.Response.Header.SetStatusCode(fasthttp.StatusOK)
	ctx.Response.SetBody([]byte("{}"))
}

// HandleReady http handler for /ready
func (s *VllmSimulator) HandleReady(ctx *fasthttp.RequestCtx) {
	s.context.logger.V(logging.TRACE).Info("Readiness request received")
	ctx.Response.Header.SetContentType("application/json")
	ctx.Response.Header.SetStatusCode(fasthttp.StatusOK)
	ctx.Response.SetBody([]byte("{}"))
}

// HandleIsSleeping handles /is_sleeping request according
func (s *VllmSimulator) HandleIsSleeping(ctx *fasthttp.RequestCtx) {
	s.context.logger.V(logging.TRACE).Info("/is_sleeping request received")

	s.sleepMutex.RLock()
	defer s.sleepMutex.RUnlock()
	data, err := json.Marshal(map[string]bool{"is_sleeping": s.isSleeping})
	if err != nil {
		s.context.logger.Error(err, "failed to marshal isSleeping response")
		ctx.Error("Failed to marshal isSleeping response, "+err.Error(), fasthttp.StatusInternalServerError)
		return
	}

	ctx.Response.Header.SetContentType("application/json")
	ctx.Response.Header.SetStatusCode(fasthttp.StatusOK)
	ctx.Response.SetBody(data)
}

// HandleSleep http handler for /sleep
func (s *VllmSimulator) HandleSleep(ctx *fasthttp.RequestCtx) {
	if s.context.config.EnableSleepMode && s.isInDevMode {
		s.context.logger.V(logging.INFO).Info("Sleep request received")
		s.sleepMutex.Lock()
		defer s.sleepMutex.Unlock()

		s.isSleeping = true
		if s.context.config.EnableKVCache {
			s.context.kvcacheHelper.Discard()
		}
	} else {
		s.context.logger.V(logging.INFO).Info("Sleep request received, skipped since simulator not in dev mode or sleep support is not enabled")
	}

	ctx.Response.Header.SetStatusCode(fasthttp.StatusOK)
}

// HandleWakeUp http handler for /wake_up
func (s *VllmSimulator) HandleWakeUp(ctx *fasthttp.RequestCtx) {
	s.context.logger.V(logging.INFO).Info("Wake up request received")

	var wakeUpKVCache bool
	tags := ctx.QueryArgs().Peek("tags")
	if tags != nil {
		if string(tags) == "kv_cache" {
			wakeUpKVCache = true
		}
	} else {
		wakeUpKVCache = true
	}

	s.sleepMutex.Lock()
	defer s.sleepMutex.Unlock()

	// Activate the kv cache if either the tags are "kv_cache" or there are no tags
	if s.context.config.EnableKVCache && wakeUpKVCache {
		s.context.kvcacheHelper.Activate()
	}

	s.isSleeping = false

	ctx.Response.Header.SetStatusCode(fasthttp.StatusOK)
}
